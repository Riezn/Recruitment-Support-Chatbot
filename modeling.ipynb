{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Judan Syamsul\n",
      "[nltk_data]     Hadad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json file\n",
    "f = open('intent/intent.json', 'r')\n",
    "intent_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list from json\n",
    "input = []\n",
    "intent = []\n",
    "\n",
    "for i in range(len(intent_json['intents'])):\n",
    "    for user_input in intent_json['intents'][i]['input']:\n",
    "        input.append(user_input)\n",
    "        intent.append(intent_json['intents'][i]['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>halo</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hei</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hallo</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input intent\n",
       "0   halo   sapa\n",
       "1    hei   sapa\n",
       "2    hey   sapa\n",
       "3  hello   sapa\n",
       "4  hallo   sapa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe from json\n",
    "df = pd.DataFrame({\n",
    "    'input': input,\n",
    "    'intent' : intent\n",
    "    # 'response' : response\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stemmer and stopword remover\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang = pd.read_csv('lexicon/slang ke semi baku.csv')\n",
    "\n",
    "slang_replace = {}\n",
    "for i, row in enumerate(slang['slang']):\n",
    "    slang_replace[row] = slang['formal'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "baku = pd.read_csv('lexicon/slang ke baku.csv')\n",
    "\n",
    "baku_replace = {}\n",
    "for i, row in enumerate(baku['slang']):\n",
    "    baku_replace[row] = baku['baku'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text cleaning function\n",
    "def clean_text(text):\n",
    "    new_text = []\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    for kata in text.split():\n",
    "        if kata not in (slang_replace|baku_replace):\n",
    "            new_text.append(kata)\n",
    "        elif kata in baku_replace:\n",
    "            new_text+=baku_replace[kata].split()\n",
    "        elif kata in slang_replace:\n",
    "            new_text+=slang_replace[kata].split()\n",
    "    new_text = ' '.join(\n",
    "        stemmer.stem(\n",
    "            baku_replace.get(\n",
    "                word,\n",
    "                word\n",
    "            )\n",
    "        ) for word in new_text if word not in stopwords\n",
    "    )\n",
    "\n",
    "    new_text = new_text.translate(\n",
    "        str.maketrans(\n",
    "            '',\n",
    "            '',\n",
    "            string.punctuation\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aku lapar banget tahu mau makan jeruk apa tiru deh lamar'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat = 'Aku laper banget gatau maunya makan jeruk apa lagi meniru-niru... daah ngelamar'\n",
    "clean_text(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     halo\n",
       "1      hei\n",
       "2      hai\n",
       "3    hello\n",
       "4     halo\n",
       "Name: clean_input, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_input'] = df['input'].apply(clean_text)\n",
    "df['clean_input'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus\n",
    "words = set([\n",
    "    word for word in df['clean_input'] for word in word_tokenize(word)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word length of each row\n",
    "df['length'] = df['clean_input'].apply(word_tokenize).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = int(round(df['length'].max(),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_size, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df['intent'])\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['biaya', 'daftar', 'dokumen', 'error upload', 'ktp', 'link sosmed',\n",
       "       'lowongan', 'lupa password', 'nama', 'pas foto', 'pengantar',\n",
       "       'penutup', 'qualification', 'responsibilities', 'salary', 'sapa',\n",
       "       'sertifikat', 'skck', 'timeline', 'training', 'transkrip-ijazah'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvect = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=corpus_size,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "textvect.adapt(df['clean_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=corpus_size,\n",
    "    output_dim=16,\n",
    "    input_length=sequence_length,\n",
    "    embeddings_initializer='uniform'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([20, 11,  7,  0,  0,  0,  0,  0], dtype=int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes = 'saya mau daftar rekrutmen'\n",
    "textvect(clean_text(tes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 16), dtype=float32, numpy=\n",
       "array([[-0.03436317,  0.03928449,  0.02017006, -0.02457641,  0.01162897,\n",
       "         0.03741631, -0.00135561,  0.03635332, -0.00161115, -0.04203521,\n",
       "         0.01314846, -0.02541434, -0.01312237, -0.00419213, -0.02114015,\n",
       "        -0.03910436],\n",
       "       [ 0.01110385, -0.02799114,  0.01052301, -0.02551323,  0.02929792,\n",
       "         0.04521887,  0.02336918,  0.0429708 , -0.01190245, -0.00029536,\n",
       "        -0.04396411,  0.03737772, -0.00811651,  0.01045377,  0.03049422,\n",
       "        -0.02706285],\n",
       "       [-0.03663881, -0.04771641, -0.02090973, -0.03309351, -0.03991692,\n",
       "        -0.02790138, -0.0454954 , -0.01770438, -0.03191983, -0.02283714,\n",
       "        -0.01921368, -0.01275697,  0.00446593,  0.02818007, -0.03166701,\n",
       "        -0.0173602 ],\n",
       "       [-0.02534754, -0.0019244 , -0.0275056 ,  0.02017946,  0.04213474,\n",
       "         0.00129261,  0.01603736, -0.01392318,  0.01804024, -0.00492246,\n",
       "        -0.00227969, -0.0392528 , -0.0365636 ,  0.04154776, -0.03142873,\n",
       "        -0.01164029],\n",
       "       [-0.02534754, -0.0019244 , -0.0275056 ,  0.02017946,  0.04213474,\n",
       "         0.00129261,  0.01603736, -0.01392318,  0.01804024, -0.00492246,\n",
       "        -0.00227969, -0.0392528 , -0.0365636 ,  0.04154776, -0.03142873,\n",
       "        -0.01164029],\n",
       "       [-0.02534754, -0.0019244 , -0.0275056 ,  0.02017946,  0.04213474,\n",
       "         0.00129261,  0.01603736, -0.01392318,  0.01804024, -0.00492246,\n",
       "        -0.00227969, -0.0392528 , -0.0365636 ,  0.04154776, -0.03142873,\n",
       "        -0.01164029],\n",
       "       [-0.02534754, -0.0019244 , -0.0275056 ,  0.02017946,  0.04213474,\n",
       "         0.00129261,  0.01603736, -0.01392318,  0.01804024, -0.00492246,\n",
       "        -0.00227969, -0.0392528 , -0.0365636 ,  0.04154776, -0.03142873,\n",
       "        -0.01164029],\n",
       "       [-0.02534754, -0.0019244 , -0.0275056 ,  0.02017946,  0.04213474,\n",
       "         0.00129261,  0.01603736, -0.01392318,  0.01804024, -0.00492246,\n",
       "        -0.00227969, -0.0392528 , -0.0365636 ,  0.04154776, -0.03142873,\n",
       "        -0.01164029]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(textvect(clean_text(tes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "input = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "hidden_layer = textvect(input)\n",
    "hidden_layer = embedding(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16))(hidden_layer)\n",
    "output = tf.keras.layers.Dense(len(le.classes_), activation='softmax')(hidden_layer)\n",
    "model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.020370</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.019977</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.019746</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.019094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.018647</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  categorical_accuracy\n",
       "95  0.020370                   1.0\n",
       "96  0.019977                   1.0\n",
       "97  0.019746                   1.0\n",
       "98  0.019094                   1.0\n",
       "99  0.018647                   1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = model.fit(df['clean_input'], y_train, epochs=100, verbose=0)\n",
    "pd.DataFrame(hist.history).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 2s 3ms/step - loss: 0.0182 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.018221931532025337, 1.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(df['clean_input'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           biaya       1.00      1.00      1.00        27\n",
      "          daftar       1.00      1.00      1.00        22\n",
      "         dokumen       1.00      1.00      1.00        22\n",
      "    error upload       1.00      1.00      1.00         9\n",
      "             ktp       1.00      1.00      1.00        15\n",
      "     link sosmed       1.00      1.00      1.00        10\n",
      "        lowongan       1.00      1.00      1.00        12\n",
      "   lupa password       1.00      1.00      1.00        21\n",
      "            nama       1.00      1.00      1.00         8\n",
      "        pas foto       1.00      1.00      1.00        15\n",
      "       pengantar       1.00      1.00      1.00        10\n",
      "         penutup       1.00      1.00      1.00        21\n",
      "   qualification       1.00      1.00      1.00        16\n",
      "responsibilities       1.00      1.00      1.00        21\n",
      "          salary       1.00      1.00      1.00        35\n",
      "            sapa       1.00      1.00      1.00        19\n",
      "      sertifikat       1.00      1.00      1.00        14\n",
      "            skck       1.00      1.00      1.00        16\n",
      "        timeline       1.00      1.00      1.00        38\n",
      "        training       1.00      1.00      1.00        22\n",
      "transkrip-ijazah       1.00      1.00      1.00        29\n",
      "\n",
      "        accuracy                           1.00       402\n",
      "       macro avg       1.00      1.00      1.00       402\n",
      "    weighted avg       1.00      1.00      1.00       402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_df = pd.DataFrame(y_train, columns=le.classes_)\n",
    "y_train_df['intent'] = y_train_df.idxmax(axis=1)\n",
    "\n",
    "model_pred = model.predict(df['clean_input'])\n",
    "model_pred = pd.DataFrame(model_pred, columns=le.classes_)\n",
    "model_pred['intent'] = model_pred.idxmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train_df['intent'], model_pred['intent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(text):\n",
    "    \"\"\"Take text as function input then predict using model. Return response based on highest probability using numpy argmax    \n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    pred = model.predict([text])\n",
    "    res = le.classes_[pred.argmax()]\n",
    "    if textvect(text).numpy().max() > 1:\n",
    "        for label_pred in intent_json['intents']:\n",
    "            if label_pred['intent'] == res:\n",
    "                response = label_pred['response']\n",
    "    else:\n",
    "        response = ['Maaf, saya tidak mengerti']\n",
    "    \n",
    "    dict_temp = []\n",
    "    for i in range(len(pred[0])):\n",
    "        temp = {le.classes_[i]: pred[0][i]}\n",
    "        dict_temp.append(temp)\n",
    "    print(dict_temp)\n",
    "    print(le.classes_[pred.argmax()])\n",
    "    return print(np.random.choice(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(le, open('saved_model/encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump({'config': textvect.get_config(),\n",
    "#              'weights': textvect.get_weights()}\n",
    "#             , open(\"saved_model/textvect.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002C8EFBB5B80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002C8EF827A00> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# model.save('saved_model/model', save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'biaya': 0.019276958}, {'daftar': 0.010527545}, {'dokumen': 0.00040877648}, {'error upload': 0.00030921155}, {'ktp': 5.707197e-05}, {'link sosmed': 0.00015870223}, {'lowongan': 2.9218145e-05}, {'lupa password': 0.5926877}, {'nama': 0.0008345293}, {'pas foto': 0.0025887717}, {'pengantar': 4.8192658e-05}, {'penutup': 0.0013467034}, {'qualification': 0.0025026738}, {'responsibilities': 0.0012988395}, {'salary': 8.8033805e-05}, {'sapa': 0.001593638}, {'sertifikat': 0.0962988}, {'skck': 0.0012394362}, {'timeline': 0.008503553}, {'training': 0.24781552}, {'transkrip-ijazah': 0.012386126}]\n",
      "lupa password\n",
      "Maaf, saya tidak mengerti\n"
     ]
    }
   ],
   "source": [
    "bot_response('laper pengen makan jeruk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'biaya': 0.00028879038}, {'daftar': 3.446055e-05}, {'dokumen': 0.0013081614}, {'error upload': 2.2282444e-05}, {'ktp': 0.0004544905}, {'link sosmed': 8.479538e-05}, {'lowongan': 3.4720672e-06}, {'lupa password': 0.00045636593}, {'nama': 0.00019217581}, {'pas foto': 0.9855884}, {'pengantar': 2.5535815e-05}, {'penutup': 0.0035906234}, {'qualification': 2.462237e-05}, {'responsibilities': 0.00014229809}, {'salary': 7.912264e-06}, {'sapa': 0.00042383056}, {'sertifikat': 0.0009924851}, {'skck': 0.004791357}, {'timeline': 3.6891788e-06}, {'training': 5.272255e-05}, {'transkrip-ijazah': 0.0015113606}]\n",
      "pas foto\n",
      "Pas foto merupakan dokumen wajib pendaftaran ya, Kak. Berikut ketentuan pas foto yang harus dipenuhi:\n",
      "- Background berwarna biru dengan pakaian formal (kemeja dan/atau jas)\n",
      "- Foto berukuran 3x4 dan berwarna\n",
      "- Disarankan untuk menggunakan foto terbaru\n",
      "- Upload foto di https://rekrutmen.fiktif.id/dokumen dengan ukuran file tidak lebih dari 1 MB dengan format file jpg/png/jpeg\n"
     ]
    }
   ],
   "source": [
    "bot_response('pas foto hilang gimana ya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ktp, pas foto, password masih ketuker2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['biaya', 'daftar', 'dokumen', 'error upload', 'ktp', 'link sosmed',\n",
       "       'lowongan', 'lupa password', 'nama', 'pas foto', 'pengantar',\n",
       "       'penutup', 'qualification', 'responsibilities', 'salary', 'sapa',\n",
       "       'sertifikat', 'skck', 'timeline', 'training', 'transkrip-ijazah'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87147628ceb55968efe4bb59da7a0e7c4394b9758046b539c4601b1e67cb397e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
