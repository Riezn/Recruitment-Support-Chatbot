{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General\n",
    "import json\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# Word Preprocessing\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt') # Only needed for first time usage of NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files for Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json file\n",
    "f = open('intent/intent.json', 'r')\n",
    "intent_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list from json\n",
    "input = []\n",
    "intent = []\n",
    "\n",
    "for i in range(len(intent_json['intents'])):\n",
    "    for user_input in intent_json['intents'][i]['input']:\n",
    "        input.append(user_input)\n",
    "        intent.append(intent_json['intents'][i]['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>halo</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hei</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hallo</td>\n",
       "      <td>sapa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input intent\n",
       "0   halo   sapa\n",
       "1    hei   sapa\n",
       "2    hey   sapa\n",
       "3  hello   sapa\n",
       "4  hallo   sapa"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe from json\n",
    "df = pd.DataFrame({\n",
    "    'input': input,\n",
    "    'intent' : intent\n",
    "    # 'response' : response\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stemmer and stopword\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.remove('ok')\n",
    "stopwords.remove('oh')\n",
    "stopwords.remove('tidak')\n",
    "stopwords.remove('ya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slang = pd.read_csv('lexicon/slang ke semi baku.csv')\n",
    "\n",
    "# slang_replace = {}\n",
    "# for i, row in enumerate(slang['slang']):\n",
    "#     slang_replace[row] = slang['formal'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baku = pd.read_csv('lexicon/slang ke baku.csv')\n",
    "\n",
    "# std_word_replace = {}\n",
    "# for i, row in enumerate(baku['slang']):\n",
    "#     std_word_replace[row] = baku['baku'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "baku = pd.read_csv('lexicon/baku.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_word_replace = {}\n",
    "for i, row in enumerate(baku['slang']):\n",
    "    std_word_replace[row] = baku['baku'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tidak'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_word_replace['gak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text cleaning function\n",
    "def clean_text(text):\n",
    "    new_text = []\n",
    "    text = text.lower() # Lowercase\n",
    "    # Loop each word in a sentence\n",
    "    for kata in text.split(): \n",
    "        # Keep word not in slang or standard word\n",
    "        if kata not in std_word_replace: \n",
    "            new_text.append(kata) \n",
    "        # Replace non-formal word with standard word\n",
    "        elif kata in std_word_replace:\n",
    "            new_text+=std_word_replace[kata].split() \n",
    "    # Join words without stopwords after stemming\n",
    "    new_text = ' '.join(\n",
    "        stemmer.stem(word) for word in new_text if word not in stopwords\n",
    "    )\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aku lapar banget tidak tahu mau makan jeruk apa tiru deh lamar'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat = 'Aku laper banget gatau maunya makan jeruk apa lagi meniru-niru... daah ngelamar'\n",
    "clean_text(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     halo\n",
       "1      hei\n",
       "2      hai\n",
       "3    hello\n",
       "4     halo\n",
       "Name: clean_input, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_input'] = df['input'].apply(clean_text)\n",
    "df['clean_input'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus\n",
    "words = set([\n",
    "    word for word in df['clean_input'] for word in word_tokenize(word)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word length of each row\n",
    "df['length'] = df['clean_input'].apply(word_tokenize).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = int(round(df['length'].max(),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 8)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_size, sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df['intent'])\n",
    "y_train = tf.keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CV', 'biaya', 'daftar', 'dokumen', 'error upload', 'ktp',\n",
       "       'link sosmed', 'lowongan', 'lupa password', 'nama', 'pas foto',\n",
       "       'pengantar', 'penutup', 'qualification', 'responsibilities',\n",
       "       'salary', 'sapa', 'sertifikat', 'skck', 'timeline', 'training',\n",
       "       'transkrip-ijazah'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvect = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=corpus_size,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "textvect.adapt(df['clean_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([23, 12, 11,  0,  0,  0,  0,  0], dtype=int64)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes = 'saya mau daftar rekrutmen'\n",
    "textvect(clean_text(tes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=corpus_size,\n",
    "    output_dim=16,\n",
    "    input_length=sequence_length,\n",
    "    embeddings_initializer='uniform'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 16), dtype=float32, numpy=\n",
       "array([[ 0.03853462, -0.03360705,  0.02211213, -0.01795112, -0.03602955,\n",
       "         0.04949274,  0.0031283 ,  0.03274949, -0.02107451,  0.02702581,\n",
       "        -0.03820109,  0.0433121 ,  0.00571074, -0.01101174,  0.01575703,\n",
       "         0.0268165 ],\n",
       "       [ 0.02885098, -0.03947011,  0.01563777, -0.02747667, -0.03002098,\n",
       "        -0.00772154,  0.00211558, -0.00292844,  0.00856432,  0.0006563 ,\n",
       "        -0.0308546 ,  0.02973196, -0.01230348, -0.02144375, -0.02638157,\n",
       "        -0.03389976],\n",
       "       [ 0.02081262, -0.04149457,  0.04172282,  0.02582288, -0.02856763,\n",
       "        -0.01862653,  0.00862879,  0.03750468,  0.04291407, -0.00556409,\n",
       "         0.02454365,  0.01561158,  0.01999387,  0.03208322, -0.04166334,\n",
       "         0.01679399],\n",
       "       [-0.04628502,  0.00121652, -0.02749063, -0.01016929,  0.04322923,\n",
       "        -0.01837881, -0.01531589,  0.02788894,  0.02633411,  0.00433249,\n",
       "        -0.00319986,  0.01254359, -0.04987613, -0.00223448,  0.02571288,\n",
       "         0.03803735],\n",
       "       [-0.04628502,  0.00121652, -0.02749063, -0.01016929,  0.04322923,\n",
       "        -0.01837881, -0.01531589,  0.02788894,  0.02633411,  0.00433249,\n",
       "        -0.00319986,  0.01254359, -0.04987613, -0.00223448,  0.02571288,\n",
       "         0.03803735],\n",
       "       [-0.04628502,  0.00121652, -0.02749063, -0.01016929,  0.04322923,\n",
       "        -0.01837881, -0.01531589,  0.02788894,  0.02633411,  0.00433249,\n",
       "        -0.00319986,  0.01254359, -0.04987613, -0.00223448,  0.02571288,\n",
       "         0.03803735],\n",
       "       [-0.04628502,  0.00121652, -0.02749063, -0.01016929,  0.04322923,\n",
       "        -0.01837881, -0.01531589,  0.02788894,  0.02633411,  0.00433249,\n",
       "        -0.00319986,  0.01254359, -0.04987613, -0.00223448,  0.02571288,\n",
       "         0.03803735],\n",
       "       [-0.04628502,  0.00121652, -0.02749063, -0.01016929,  0.04322923,\n",
       "        -0.01837881, -0.01531589,  0.02788894,  0.02633411,  0.00433249,\n",
       "        -0.00319986,  0.01254359, -0.04987613, -0.00223448,  0.02571288,\n",
       "         0.03803735]], dtype=float32)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(textvect(clean_text(tes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "input = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "hidden_layer = textvect(input)\n",
    "hidden_layer = embedding(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16))(hidden_layer)\n",
    "output = tf.keras.layers.Dense(len(le.classes_), activation='softmax')(hidden_layer)\n",
    "model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.019533</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.019120</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.018803</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.018251</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.017757</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  categorical_accuracy\n",
       "95  0.019533                   1.0\n",
       "96  0.019120                   1.0\n",
       "97  0.018803                   1.0\n",
       "98  0.018251                   1.0\n",
       "99  0.017757                   1.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = model.fit(df['clean_input'], y_train, epochs=100, verbose=0)\n",
    "pd.DataFrame(hist.history).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 4s 4ms/step - loss: 0.0174 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01736104115843773, 1.0]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(df['clean_input'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "              CV       1.00      1.00      1.00        13\n",
      "           biaya       1.00      1.00      1.00        28\n",
      "          daftar       1.00      1.00      1.00        22\n",
      "         dokumen       1.00      1.00      1.00        23\n",
      "    error upload       1.00      1.00      1.00        15\n",
      "             ktp       1.00      1.00      1.00        18\n",
      "     link sosmed       1.00      1.00      1.00        45\n",
      "        lowongan       1.00      1.00      1.00        22\n",
      "   lupa password       1.00      1.00      1.00        24\n",
      "            nama       1.00      1.00      1.00        10\n",
      "        pas foto       1.00      1.00      1.00        23\n",
      "       pengantar       1.00      1.00      1.00        17\n",
      "         penutup       1.00      1.00      1.00        50\n",
      "   qualification       1.00      1.00      1.00        26\n",
      "responsibilities       1.00      1.00      1.00        35\n",
      "          salary       1.00      1.00      1.00        38\n",
      "            sapa       1.00      1.00      1.00        21\n",
      "      sertifikat       1.00      1.00      1.00        14\n",
      "            skck       1.00      1.00      1.00        17\n",
      "        timeline       1.00      1.00      1.00        53\n",
      "        training       1.00      1.00      1.00        28\n",
      "transkrip-ijazah       1.00      1.00      1.00        37\n",
      "\n",
      "        accuracy                           1.00       579\n",
      "       macro avg       1.00      1.00      1.00       579\n",
      "    weighted avg       1.00      1.00      1.00       579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_df = pd.DataFrame(y_train, columns=le.classes_)\n",
    "y_train_df['intent'] = y_train_df.idxmax(axis=1)\n",
    "\n",
    "model_pred = model.predict(df['clean_input'])\n",
    "model_pred = pd.DataFrame(model_pred, columns=le.classes_)\n",
    "model_pred['intent'] = model_pred.idxmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train_df['intent'], model_pred['intent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Covering End-to-End Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(text):\n",
    "    \"\"\"Take text as function input then predict using model. Return response based on highest probability using numpy argmax    \n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    pred = model.predict([text])\n",
    "    res = le.classes_[pred.argmax()] # Get the index with highest probability\n",
    "    i = 0\n",
    "    try:\n",
    "        if textvect(text).numpy().max() > 1: # If the input is known word(s)\n",
    "            while i < len(intent_json['intents']):\n",
    "                if res == intent_json['intents'][i]['intent']:\n",
    "                    response = intent_json['intents'][i]['response']\n",
    "                    break\n",
    "                else:\n",
    "                    i+=1\n",
    "        else: # If only unknown word(s)\n",
    "            response = ['Maaf, Kak. Aku tidak mengerti...']\n",
    "    except: # If empty string or any error occured\n",
    "        response = ['Maaf, Kak. Aku tidak mengerti...']\n",
    "\n",
    "    # For debugging only\n",
    "    dict_temp = []\n",
    "    for i in range(len(pred[0])):\n",
    "        temp = {le.classes_[i]: pred[0][i]}\n",
    "        dict_temp.append(temp)\n",
    "    print(dict_temp)\n",
    "    print(le.classes_[pred.argmax()])\n",
    "\n",
    "    return print(np.random.choice(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aku mau daftar rekrutmen'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes1 = 'aku mau daftar rekrutmen'\n",
    "clean_text(tes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.08448342e-04, 1.09997904e-06, 9.89931822e-01, 2.39695466e-04,\n",
       "        8.31362093e-04, 2.15991793e-04, 2.99815838e-05, 5.90342861e-05,\n",
       "        1.36565045e-03, 1.17290610e-05, 1.74880624e-04, 3.64732332e-05,\n",
       "        4.49598127e-04, 1.51047728e-03, 9.22848049e-05, 1.06580956e-05,\n",
       "        3.40960505e-05, 5.65641074e-07, 4.04407829e-03, 1.08255745e-05,\n",
       "        8.88806198e-07, 7.40399002e-04]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([clean_text(tes1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([clean_text(tes1)]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daftar'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_[model.predict([clean_text(tes1)]).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_[model.predict([clean_text(tes1)]).argmax()] == intent_json['intents'][2]['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Untuk mengikuti rekrutmen ini, mohon ikuti langkah berikut ya, Kak:\\n1. Baca tata cara melamar di https://rekrutmen.fiktif.id/tatacara\\n2. Buat akun di https://rekrutmen.fiktif.id/registrasi menggunakan alamat email dan nomor telepon aktif milik pribadi\\n3. Lengkapi formulir pendaftaran dan upload seluruh dokumen yang dibutuhkan dan tunggu pengumuman tahap berikutnya di https://rekrutmen.fiktif.id/news atau email masing-masing']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_json['intents'][2]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'CV': 0.00020844834}, {'biaya': 1.099979e-06}, {'daftar': 0.9899318}, {'dokumen': 0.00023969547}, {'error upload': 0.0008313621}, {'ktp': 0.0002159918}, {'link sosmed': 2.9981584e-05}, {'lowongan': 5.9034286e-05}, {'lupa password': 0.0013656504}, {'nama': 1.1729061e-05}, {'pas foto': 0.00017488062}, {'pengantar': 3.6473233e-05}, {'penutup': 0.00044959813}, {'qualification': 0.0015104773}, {'responsibilities': 9.2284805e-05}, {'salary': 1.0658096e-05}, {'sapa': 3.409605e-05}, {'sertifikat': 5.656411e-07}, {'skck': 0.0040440783}, {'timeline': 1.08255745e-05}, {'training': 8.888062e-07}, {'transkrip-ijazah': 0.000740399}]\n",
      "daftar\n",
      "Untuk mengikuti rekrutmen ini, mohon ikuti langkah berikut ya, Kak:\n",
      "1. Baca tata cara melamar di https://rekrutmen.fiktif.id/tatacara\n",
      "2. Buat akun di https://rekrutmen.fiktif.id/registrasi menggunakan alamat email dan nomor telepon aktif milik pribadi\n",
      "3. Lengkapi formulir pendaftaran dan upload seluruh dokumen yang dibutuhkan dan tunggu pengumuman tahap berikutnya di https://rekrutmen.fiktif.id/news atau email masing-masing\n"
     ]
    }
   ],
   "source": [
    "bot_response('aku mau daftar rekrutmen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(le, open('saved_model/encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'config': textvect.get_config(),\n",
    "             'weights': textvect.get_weights()}\n",
    "            , open(\"saved_model/textvect.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000241EFF8F8B0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000241F6777F40> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/model', save_traces=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([279,   0,   0,   0,   0,   0,   0,   0], dtype=int64)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textvect('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textvect('ok').numpy().max() > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'CV': 0.00014926693}, {'biaya': 0.0009817937}, {'daftar': 0.0029506683}, {'dokumen': 0.004454021}, {'error upload': 1.0452405e-05}, {'ktp': 1.5375803e-06}, {'link sosmed': 0.0049550817}, {'lowongan': 0.0005463395}, {'lupa password': 0.009204468}, {'nama': 2.383838e-05}, {'pas foto': 0.018794892}, {'pengantar': 5.5192544e-05}, {'penutup': 0.4635196}, {'qualification': 0.0005873692}, {'responsibilities': 4.0081006e-05}, {'salary': 0.024214271}, {'sapa': 0.005021482}, {'sertifikat': 1.5208798e-05}, {'skck': 5.2372266e-06}, {'timeline': 0.45977518}, {'training': 0.0006664098}, {'transkrip-ijazah': 0.0040275985}]\n",
      "penutup\n",
      "Maaf, Kak. Aku tidak mengerti...\n"
     ]
    }
   ],
   "source": [
    "bot_response('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'CV': 4.8645627e-05}, {'biaya': 9.297839e-05}, {'daftar': 8.39374e-05}, {'dokumen': 0.00059122563}, {'error upload': 1.2777886e-06}, {'ktp': 2.6287344e-08}, {'link sosmed': 0.0010752971}, {'lowongan': 1.7060756e-05}, {'lupa password': 0.00033030257}, {'nama': 1.5115894e-06}, {'pas foto': 0.00044017186}, {'pengantar': 3.206031e-06}, {'penutup': 0.9921267}, {'qualification': 0.00028312803}, {'responsibilities': 3.4790216e-06}, {'salary': 0.00067263364}, {'sapa': 0.0012251198}, {'sertifikat': 2.5750117e-07}, {'skck': 2.912159e-06}, {'timeline': 0.0028858148}, {'training': 2.262961e-05}, {'transkrip-ijazah': 9.157889e-05}]\n",
      "penutup\n",
      "Terima kasih telah bertanya. Senang bisa membantumu :D\n"
     ]
    }
   ],
   "source": [
    "bot_response('ya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jeruk',\n",
       " <tf.Tensor: shape=(8,), dtype=int64, numpy=array([1, 0, 0, 0, 0, 0, 0, 0], dtype=int64)>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testext = 'jeruk'\n",
    "clean_text(testext), textvect(clean_text(testext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textvect(clean_text('laper pengen makan jeruk')).numpy().max() > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'CV': 3.94773e-05}, {'biaya': 1.8849532e-05}, {'daftar': 0.0007545028}, {'dokumen': 0.01639448}, {'error upload': 5.526689e-06}, {'ktp': 4.024321e-07}, {'link sosmed': 0.0007895099}, {'lowongan': 4.71152e-06}, {'lupa password': 0.0027690588}, {'nama': 5.6055637e-06}, {'pas foto': 0.0004012197}, {'pengantar': 2.196414e-06}, {'penutup': 0.9726863}, {'qualification': 0.0044182516}, {'responsibilities': 2.1075653e-05}, {'salary': 0.00034652805}, {'sapa': 0.00042660194}, {'sertifikat': 8.289796e-08}, {'skck': 7.781334e-05}, {'timeline': 0.0004984812}, {'training': 4.7245862e-06}, {'transkrip-ijazah': 0.00033458098}]\n",
      "penutup\n",
      "Terima kasih telah bertanya. Senang bisa membantumu :D\n"
     ]
    }
   ],
   "source": [
    "bot_response('oh ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'CV': 0.0013804277}, {'biaya': 0.0006941786}, {'daftar': 0.00023378422}, {'dokumen': 2.6587766e-05}, {'error upload': 0.012864039}, {'ktp': 0.00032194174}, {'link sosmed': 0.0056037037}, {'lowongan': 0.83321065}, {'lupa password': 6.057517e-07}, {'nama': 1.613094e-05}, {'pas foto': 9.318026e-05}, {'pengantar': 0.13507889}, {'penutup': 1.0898733e-05}, {'qualification': 0.00030367819}, {'responsibilities': 0.006136759}, {'salary': 0.001077325}, {'sapa': 0.00030397807}, {'sertifikat': 0.0016794958}, {'skck': 7.488825e-06}, {'timeline': 2.3474164e-05}, {'training': 0.0009315298}, {'transkrip-ijazah': 1.2752273e-06}]\n",
      "lowongan\n",
      "Saat ini PT Fiktif sedang membuka lowongan untuk posisi Data Analyst, Data Scientist, dan Data Engineer. Yuk daftar lewat https://rekrutmen.fiktif.id/\n"
     ]
    }
   ],
   "source": [
    "bot_response('ada lowongan apa nih di pt fiktif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'CV': 0.003968113}, {'biaya': 0.024110552}, {'daftar': 0.00024694053}, {'dokumen': 0.00066814624}, {'error upload': 8.429675e-05}, {'ktp': 5.4167863e-06}, {'link sosmed': 0.3084902}, {'lowongan': 0.017921079}, {'lupa password': 0.00015259824}, {'nama': 0.0006130161}, {'pas foto': 0.0074404273}, {'pengantar': 0.0002531179}, {'penutup': 0.36954764}, {'qualification': 0.013508225}, {'responsibilities': 0.0019993146}, {'salary': 0.011219099}, {'sapa': 0.22163841}, {'sertifikat': 5.5981087e-05}, {'skck': 0.00010695672}, {'timeline': 0.0056106234}, {'training': 0.011911318}, {'transkrip-ijazah': 0.0004485653}]\n",
      "penutup\n",
      "Maaf, Kak. Aku tidak mengerti...\n"
     ]
    }
   ],
   "source": [
    "bot_response('ingin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'CV': 7.264111e-05}, {'biaya': 8.040791e-05}, {'daftar': 0.00020830285}, {'dokumen': 1.7837606e-06}, {'error upload': 0.00029116304}, {'ktp': 9.902271e-07}, {'link sosmed': 2.4861673e-05}, {'lowongan': 0.00029221067}, {'lupa password': 0.0017457632}, {'nama': 4.1985393e-05}, {'pas foto': 0.99150956}, {'pengantar': 3.6541098e-06}, {'penutup': 0.00243599}, {'qualification': 1.523724e-05}, {'responsibilities': 4.391056e-07}, {'salary': 0.000763338}, {'sapa': 9.460553e-05}, {'sertifikat': 4.352439e-06}, {'skck': 2.5162421e-06}, {'timeline': 0.00033102836}, {'training': 0.0017707733}, {'transkrip-ijazah': 0.00030833704}]\n",
      "pas foto\n",
      "Pas foto merupakan dokumen wajib pendaftaran ya, Kak.\n",
      "\n",
      "Berikut ketentuan pas foto yang harus dipenuhi:\n",
      "- Background berwarna biru dengan pakaian formal (kemeja dan/atau jas)\n",
      "- Foto berukuran 3x4 dan berwarna\n",
      "- Disarankan untuk menggunakan foto terbaru\n",
      "- Upload foto di https://rekrutmen.fiktif.id/dokumen dengan ukuran file tidak lebih dari 1 MB dengan format file jpg/png/jpeg\n",
      "\n",
      "Kalau ukuran file melebihi, silahkan diperkecil dulu dengan aplikasi untuk edit foto.\n"
     ]
    }
   ],
   "source": [
    "bot_response('pas foto hilang gimana ya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ktp, pas foto, password masih ketuker2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CV', 'biaya', 'daftar', 'dokumen', 'error upload', 'ktp',\n",
       "       'link sosmed', 'lowongan', 'lupa password', 'nama', 'pas foto',\n",
       "       'pengantar', 'penutup', 'qualification', 'responsibilities',\n",
       "       'salary', 'sapa', 'sertifikat', 'skck', 'timeline', 'training',\n",
       "       'transkrip-ijazah'], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([114,   0,   0,   0,   0,   0,   0,   0], dtype=int64)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textvect('loker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([1, 0, 0, 0, 0, 0, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textvect('wis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'user_input':'hai hai'}\n",
    "data = json.dumps(data)\n",
    "loaded_data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tespred = str(model.predict([clean_text(loaded_data['user_input'])]).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_res = {'pred':tespred}\n",
    "tes_res = json.dumps(tes_res)\n",
    "loaded_res = json.loads(tes_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sapa'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_[int(loaded_res['pred'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hai hai'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data['user_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(str(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pas foto'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_[int(str(10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([114,   0,   0,   0,   0,   0,   0,   0], dtype=int64)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textvect('loker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuk posisi Data Engineer ada beberapa kualifikasi yang harus dipenuhi:\n",
      "1. Memiliki gelar sarjana di bidang informatika, ilmu komputer, statistika, matematika, atau bidang lain yang berhubungan\n",
      "2. Memiliki pengalaman bekerja dengan tools untuk ETL seperti AWS Glue, SSIS, Informatica, dll.\n",
      "3. Memiliki pengalaman kerja di bidang Data Engineer selama 1-3 tahun\n",
      "4. Memiliki pemahaman yang baik tentang ETL, SQL, dan noSQL\n",
      "5. Memiliki kemampuan bekerja sama, kepemimpinan, dan problem solving yang baik\n"
     ]
    }
   ],
   "source": [
    "user_input = 'apa kualifikasi engineer'\n",
    "clean_input = clean_text(user_input)\n",
    "pred = model.predict([clean_input]).argmax()\n",
    "if textvect(clean_input).numpy().max() <= 1:\n",
    "    label_idx = 1000\n",
    "else:\n",
    "    label_idx = pred \n",
    "if label_idx != 1000:\n",
    "    i = 0\n",
    "    while i < len(intent_json['intents']):\n",
    "        if le.classes_[label_idx] == intent_json['intents'][i]['intent'] and (le.classes_[label_idx] != 'responsibilities' or le.classes_[label_idx] != 'qualification'):\n",
    "            response = intent_json['intents'][i]['response']\n",
    "            break\n",
    "        elif le.classes_[label_idx] == 'responsibilities':\n",
    "            if any(x in clean_input for x in ['scientist', 'ds']):\n",
    "                response = [\"Berikut adalah tanggung jawab yang akan diberikan untuk posisi Data Scientist:\\n- Merancang dan mengembangkan berbagai solusi Machine Learning dan Deep Learning untuk meningkatkan pengalaman pengguna bagi konsumen\\n- Berkolaborasi dengan seluruh elemen bisnis dan bertanggung jawab untuk merencanakan solusi end-to-end berbasis data untuk menyelesaikan permasalahan bisnis\\n- Menjadi thinking partner bagi stakeholder lain untuk memperbaiki alur perjalanan data dan penggunaannya dalam operasional perusahaan, misal merancang proses feedback-loop atau human-in-the-loop untuk meningkatkan performa model secara berkelanjutan\"]\n",
    "                break\n",
    "            elif any(x in clean_input for x in ['engineer', 'de']):\n",
    "                response = [\"Berikut adalah tanggung jawab yang akan diberikan untuk posisi Data Engineer:\\n- Membangun dan menjaga end-to-end data pipeline dari input dan output heterogen\\n- Menangani dan mengelola data warehouse\\n- Membantu tim mentransformasikan data (ETL) dan mengembangkan proses ETL dari beberapa sumber\\n- Menganalisis dan mengorganisir data mentah \\n- Memastikan kualitas data dan integrasi data\"]\n",
    "                break\n",
    "            elif any(x in clean_input for x in ['analyst', 'da', 'analis']):\n",
    "                response = [\"Berikut adalah tanggung jawab yang akan diberikan untuk posisi Data Analyst:\\n- Mengumpulkan dan menyediakan data untuk membantu stakeholder lain meningkatkan metrik bisnis perusahaan dan retensi pelanggan\\n- Menganalisis data untuk menemukan insight yang dapat ditindaklanjuti seperti membuat funnel conversion analysis, cohort analysis, long-term trends, user segmentation, dan dapat membantu meningkatkan kinerja perusahaan dan mendukung pengambilan keputusan yang lebih baik\\n - Mengidentifikasi kebutuhan dan peluang bisnis berdasarkan data yang tersedia\"]\n",
    "                break\n",
    "        elif le.classes_[label_idx] == 'qualification':\n",
    "            if any(x in clean_input for x in ['scientist', 'ds']):\n",
    "                response = [\"Untuk posisi Data Scientist ada beberapa kualifikasi yang harus dipenuhi:\\n1. Memiliki gelar sarjana di bidang informatika, ilmu komputer, statistika, matematika, atau bidang lain yang berhubungan\\n2. Memiliki pemahaman mendasar tentang Statistika Analitik, Machine Learning, Deep Learning untuk menyelesaikan permasalahan bisnis\\n3. Memiliki pengalaman kerja di bidang Data Science selama 1-3 tahun\\n4. Memiliki pemahaman dan pengalaman tentang Big Data\\n5. Memiliki kemampuan bekerja sama, kepemimpinan, dan problem solving yang baik\"]\n",
    "                break\n",
    "            elif any(x in clean_input for x in ['engineer', 'de']):\n",
    "                response = [\"Untuk posisi Data Engineer ada beberapa kualifikasi yang harus dipenuhi:\\n1. Memiliki gelar sarjana di bidang informatika, ilmu komputer, statistika, matematika, atau bidang lain yang berhubungan\\n2. Memiliki pengalaman bekerja dengan tools untuk ETL seperti AWS Glue, SSIS, Informatica, dll.\\n3. Memiliki pengalaman kerja di bidang Data Engineer selama 1-3 tahun\\n4. Memiliki pemahaman yang baik tentang ETL, SQL, dan noSQL\\n5. Memiliki kemampuan bekerja sama, kepemimpinan, dan problem solving yang baik\"]\n",
    "                break\n",
    "            elif any(x in clean_input for x in ['analyst', 'da', 'analis']):\n",
    "                response = [\"Untuk posisi Data Analyst ada beberapa kualifikasi yang harus dipenuhi:\\n1. Memiliki gelar sarjana di bidang informatika, ilmu komputer, statistika, matematika, atau bidang lain yang berhubungan\\n2. Memiliki pemahaman mendasar tentang Statistika Analitik dan Inferensial untuk mencari peluang bisnis\\n3. Memiliki pengalaman kerja di bidang Data Analyst selama 1-3 tahun\\n4. Memiliki pemahaman dan pengalaman tentang Big Data serta visualisasi dengan tools seperti Power BI, Tableau, dll.\\n5. Memiliki kemampuan bekerja sama, kepemimpinan, dan problem solving yang baik\"]\n",
    "                break\n",
    "        else:\n",
    "            i+=1\n",
    "else:\n",
    "    response = ['Maaf, Kak. Aku tidak mengerti chatnya...']\n",
    "\n",
    "print(np.random.choice(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qualification'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_[label_idx]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27f6fea6f47ae512550f0b8facdbd035a93e1dd89633f7bf2dd00a2502c71d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
