{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json file\n",
    "f = open('intent/intent.json', 'r')\n",
    "intent_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list from json\n",
    "input = []\n",
    "intent = []\n",
    "\n",
    "for i in range(len(intent_json['intents'])):\n",
    "    for user_input in intent_json['intents'][i]['input']:\n",
    "        input.append(user_input)\n",
    "        intent.append(intent_json['intents'][i]['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dimana cari skck?</td>\n",
       "      <td>skck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bagaimana cara buat skck?</td>\n",
       "      <td>skck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apa itu skck?</td>\n",
       "      <td>skck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apakah skck saya masih berlaku?</td>\n",
       "      <td>skck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>berapa lama skck saya bisa digunakan?</td>\n",
       "      <td>skck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   input intent\n",
       "0                      dimana cari skck?   skck\n",
       "1              bagaimana cara buat skck?   skck\n",
       "2                          apa itu skck?   skck\n",
       "3        apakah skck saya masih berlaku?   skck\n",
       "4  berapa lama skck saya bisa digunakan?   skck"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe from json\n",
    "df = pd.DataFrame({\n",
    "    'input': input,\n",
    "    'intent' : intent\n",
    "    # 'response' : response\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stemmer and stopword remover\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang = pd.read_csv('lexicon/slang ke semi baku.csv')\n",
    "\n",
    "slang_replace = {}\n",
    "for i, row in enumerate(slang['slang']):\n",
    "    slang_replace[row] = slang['formal'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "baku = pd.read_csv('lexicon/slang ke baku.csv')\n",
    "\n",
    "baku_replace = {}\n",
    "for i, row in enumerate(baku['slang']):\n",
    "    baku_replace[row] = baku['baku'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text cleaning function\n",
    "def clean_text(text):\n",
    "    new_text = []\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    for kata in text.split():\n",
    "        if kata not in (slang_replace|baku_replace):\n",
    "            new_text.append(kata)\n",
    "        elif kata in baku_replace:\n",
    "            new_text+=baku_replace[kata].split()\n",
    "        elif kata in slang_replace:\n",
    "            new_text+=slang_replace[kata].split()\n",
    "    new_text = ' '.join(\n",
    "        stemmer.stem(\n",
    "            baku_replace.get(\n",
    "                word,\n",
    "                word\n",
    "            )\n",
    "        ) for word in new_text if word not in stopwords\n",
    "    )\n",
    "\n",
    "    new_text = new_text.translate(\n",
    "        str.maketrans(\n",
    "            '',\n",
    "            '',\n",
    "            string.punctuation\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aku lapar banget tahu mau makan jeruk apa tiru deh lamar'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat = 'Aku laper banget gatau maunya makan jeruk apa lagi meniru-niru... daah ngelamar'\n",
    "clean_text(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   cari skck\n",
       "1    bagaimana cara buat skck\n",
       "2                    apa skck\n",
       "3                   skck laku\n",
       "4       berapa lama skck guna\n",
       "Name: clean_input, dtype: object"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_input'] = df['input'].apply(clean_text)\n",
    "df['clean_input'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = set([\n",
    "    word for word in df['clean_input'] for word in word_tokenize(word)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word length of each row\n",
    "df['length'] = df['clean_input'].apply(word_tokenize).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = int(round(df['length'].max(),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 8)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_size, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df['intent'])\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['biaya', 'daftar', 'dokumen', 'error upload', 'ktp', 'link sosmed',\n",
       "       'lowongan', 'lupa password', 'nama_bot', 'pas foto', 'pengantar',\n",
       "       'penutup', 'qualification', 'responsibilities-DS', 'salary',\n",
       "       'sertifikat', 'skck', 'timeline', 'training', 'transkrip-ijazah'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvect = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=corpus_size,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "textvect.adapt(df['clean_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=corpus_size,\n",
    "    output_dim=16,\n",
    "    input_length=sequence_length,\n",
    "    embeddings_initializer='uniform'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([23, 12,  9,  0,  0,  0,  0,  0], dtype=int64)>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes = 'saya mau daftar rekrutmen'\n",
    "textvect(clean_text(tes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 16), dtype=float32, numpy=\n",
       "array([[ 0.04846788,  0.02628523, -0.00433416, -0.01324021,  0.01427785,\n",
       "        -0.0045706 ,  0.00649945,  0.02057404, -0.04407346,  0.01841583,\n",
       "         0.00017424, -0.00500776,  0.01837561, -0.00908165, -0.03814398,\n",
       "         0.04713846],\n",
       "       [ 0.00986294, -0.0269418 , -0.04586191,  0.04803355,  0.04992703,\n",
       "         0.00619525,  0.03795925,  0.03714288, -0.01629553,  0.02620966,\n",
       "        -0.03784236, -0.00958794,  0.02586884,  0.0065267 , -0.02424387,\n",
       "         0.03395048],\n",
       "       [ 0.02879191, -0.03145681, -0.00020306, -0.01343238,  0.03986249,\n",
       "        -0.00692382,  0.00958387,  0.04946334, -0.03756637,  0.00265688,\n",
       "        -0.03132159,  0.04292189,  0.03831853, -0.01791401, -0.01970152,\n",
       "        -0.02132745],\n",
       "       [ 0.02458557, -0.04589987, -0.01925696, -0.04408726,  0.02650769,\n",
       "        -0.01396913, -0.03519198,  0.02135784,  0.02408452,  0.00863268,\n",
       "        -0.0016515 ,  0.03531349,  0.00786199, -0.04921626, -0.01357733,\n",
       "         0.01588855],\n",
       "       [ 0.02458557, -0.04589987, -0.01925696, -0.04408726,  0.02650769,\n",
       "        -0.01396913, -0.03519198,  0.02135784,  0.02408452,  0.00863268,\n",
       "        -0.0016515 ,  0.03531349,  0.00786199, -0.04921626, -0.01357733,\n",
       "         0.01588855],\n",
       "       [ 0.02458557, -0.04589987, -0.01925696, -0.04408726,  0.02650769,\n",
       "        -0.01396913, -0.03519198,  0.02135784,  0.02408452,  0.00863268,\n",
       "        -0.0016515 ,  0.03531349,  0.00786199, -0.04921626, -0.01357733,\n",
       "         0.01588855],\n",
       "       [ 0.02458557, -0.04589987, -0.01925696, -0.04408726,  0.02650769,\n",
       "        -0.01396913, -0.03519198,  0.02135784,  0.02408452,  0.00863268,\n",
       "        -0.0016515 ,  0.03531349,  0.00786199, -0.04921626, -0.01357733,\n",
       "         0.01588855],\n",
       "       [ 0.02458557, -0.04589987, -0.01925696, -0.04408726,  0.02650769,\n",
       "        -0.01396913, -0.03519198,  0.02135784,  0.02408452,  0.00863268,\n",
       "        -0.0016515 ,  0.03531349,  0.00786199, -0.04921626, -0.01357733,\n",
       "         0.01588855]], dtype=float32)>"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(textvect(clean_text(tes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "input = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "hidden_layer = textvect(input)\n",
    "hidden_layer = embedding(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.LSTM(16)(hidden_layer)\n",
    "output = tf.keras.layers.Dense(len(le.classes_), activation='softmax')(hidden_layer)\n",
    "model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.104281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.102008</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.099924</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.097772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.095782</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  categorical_accuracy\n",
       "95  0.104281                   1.0\n",
       "96  0.102008                   1.0\n",
       "97  0.099924                   1.0\n",
       "98  0.097772                   1.0\n",
       "99  0.095782                   1.0"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = model.fit(df['clean_input'], y_train, epochs=100, verbose=0)\n",
    "pd.DataFrame(hist.history).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0946 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09455347806215286, 1.0]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(df['clean_input'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              biaya       1.00      1.00      1.00        27\n",
      "             daftar       1.00      1.00      1.00        22\n",
      "            dokumen       1.00      1.00      1.00        22\n",
      "       error upload       1.00      1.00      1.00         9\n",
      "                ktp       1.00      1.00      1.00        15\n",
      "        link sosmed       1.00      1.00      1.00        10\n",
      "           lowongan       1.00      1.00      1.00        12\n",
      "      lupa password       1.00      1.00      1.00        21\n",
      "           nama_bot       1.00      1.00      1.00         8\n",
      "           pas foto       1.00      1.00      1.00        15\n",
      "          pengantar       1.00      1.00      1.00        10\n",
      "            penutup       1.00      1.00      1.00        21\n",
      "      qualification       1.00      1.00      1.00        17\n",
      "responsibilities-DS       1.00      1.00      1.00        24\n",
      "             salary       1.00      1.00      1.00        35\n",
      "         sertifikat       1.00      1.00      1.00        14\n",
      "               skck       1.00      1.00      1.00        16\n",
      "           timeline       1.00      1.00      1.00        33\n",
      "           training       1.00      1.00      1.00        22\n",
      "   transkrip-ijazah       1.00      1.00      1.00        15\n",
      "\n",
      "           accuracy                           1.00       368\n",
      "          macro avg       1.00      1.00      1.00       368\n",
      "       weighted avg       1.00      1.00      1.00       368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_df = pd.DataFrame(y_train, columns=le.classes_)\n",
    "y_train_df['intent'] = y_train_df.idxmax(axis=1)\n",
    "\n",
    "model_pred = model.predict(df['clean_input'])\n",
    "model_pred = pd.DataFrame(model_pred, columns=le.classes_)\n",
    "model_pred['intent'] = model_pred.idxmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train_df['intent'], model_pred['intent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(text):\n",
    "    \"\"\"Take text as function input then predict using model. Return response based on highest probability using numpy argmax    \n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    pred = model.predict([text])\n",
    "    res = le.classes_[pred.argmax()]\n",
    "    if textvect(text).numpy().max() > 1:\n",
    "        for label_pred in intent_json['intents']:\n",
    "            if label_pred['intent'] == res:\n",
    "                response = label_pred['response']\n",
    "    else:\n",
    "        response = ['Maaf, saya tidak mengerti']\n",
    "    \n",
    "    dict_temp = []\n",
    "    for i in range(len(pred[0])):\n",
    "        temp = {le.classes_[i]: pred[0][i]}\n",
    "        dict_temp.append(temp)\n",
    "    print(dict_temp)\n",
    "    print(le.classes_[pred.argmax()])\n",
    "    return print(np.random.choice(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(le, open('encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.models.save_model(model, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'biaya': 7.790398e-05}, {'daftar': 0.011751859}, {'dokumen': 0.003143824}, {'error upload': 0.0014080789}, {'ktp': 7.284601e-07}, {'link sosmed': 0.00070906815}, {'lowongan': 0.014835751}, {'lupa password': 0.00072696636}, {'nama_bot': 6.016228e-05}, {'pas foto': 4.7655867e-05}, {'pengantar': 5.7353565e-05}, {'penutup': 0.0005667135}, {'qualification': 0.0008448163}, {'responsibilities-DS': 0.017472424}, {'salary': 0.0062029967}, {'sertifikat': 0.91824764}, {'skck': 0.00037973726}, {'timeline': 0.022906782}, {'training': 0.0005183554}, {'transkrip-ijazah': 4.112171e-05}]\n",
      "15\n",
      "sertifikat\n",
      "Maaf, saya tidak mengerti\n"
     ]
    }
   ],
   "source": [
    "bot_response('laper pengen makan jeruk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'biaya': 0.0018515236}, {'daftar': 0.00040320226}, {'dokumen': 4.5594323e-05}, {'error upload': 0.02319791}, {'ktp': 0.012349861}, {'link sosmed': 0.029984482}, {'lowongan': 0.00340934}, {'lupa password': 4.912934e-06}, {'nama_bot': 0.00013604735}, {'pas foto': 0.9051894}, {'pengantar': 0.015934302}, {'penutup': 0.0002913658}, {'qualification': 0.0008499332}, {'responsibilities-DS': 0.0015388916}, {'salary': 1.977861e-05}, {'sertifikat': 6.785532e-05}, {'skck': 8.483039e-05}, {'timeline': 0.000546083}, {'training': 0.0027302864}, {'transkrip-ijazah': 0.0013644788}]\n",
      "9\n",
      "pas foto\n",
      "Pas foto merupakan dokumen wajib pendaftaran. Berikut ketentuan pas foto yang harus dipenuhi ya, Kak:\n",
      "- Background berwarna biru dengan pakaian formal (kemeja dan/atau jas)\n",
      "- Foto berukuran 3x4 dan berwarna\n",
      "- Disarankan untuk menggunakan foto terbaru\n",
      "- Upload foto di https://rekrutmen.fiktif.id/dokumen dengan ukuran file tidak lebih dari 1 MB dengan format file jpg/png/jpeg\n"
     ]
    }
   ],
   "source": [
    "bot_response('pas foto hilang gimana ya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ktp, pas foto, password masih ketuker2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['biaya', 'daftar', 'dokumen', 'error upload', 'ktp', 'link sosmed',\n",
       "       'lowongan', 'lupa password', 'nama_bot', 'pas foto', 'pengantar',\n",
       "       'penutup', 'qualification', 'responsibilities-DS', 'salary',\n",
       "       'sertifikat', 'skck', 'timeline', 'training', 'transkrip-ijazah'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c524142758e41ff8da72ba142ad7f8c770fa04ceb078ed7556ced5f6ce4ae027"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
